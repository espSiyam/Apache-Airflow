[2023-11-25T21:00:29.260+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:00:29.266+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:00:29.266+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:00:29.277+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:00:29.282+0000] {standard_task_runner.py:57} INFO - Started process 78 to run task
[2023-11-25T21:00:29.284+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '379', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmp_4beymza']
[2023-11-25T21:00:29.286+0000] {standard_task_runner.py:85} INFO - Job 379: Subtask load_to_gcp
[2023-11-25T21:00:29.317+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:00:29.368+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:00:29.369+0000] {helper.py:74} INFO - File news_data.csv uploaded to etl_pipeline_practice successfully!
[2023-11-25T21:00:29.369+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-11-25T21:00:29.375+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T210029, end_date=20231125T210029
[2023-11-25T21:00:29.416+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-11-25T21:00:29.429+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:03:34.670+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:03:34.677+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:03:34.677+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:03:34.688+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:03:34.694+0000] {standard_task_runner.py:57} INFO - Started process 135 to run task
[2023-11-25T21:03:34.697+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '384', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmp8h1mht_9']
[2023-11-25T21:03:34.699+0000] {standard_task_runner.py:85} INFO - Job 384: Subtask load_to_gcp
[2023-11-25T21:03:34.735+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:03:34.788+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:03:34.789+0000] {helper.py:74} INFO - File news_data.csv uploaded to etl_pipeline_practice successfully!
[2023-11-25T21:03:34.789+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-11-25T21:03:34.796+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T210334, end_date=20231125T210334
[2023-11-25T21:03:34.829+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-11-25T21:03:34.847+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:38:23.725+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:38:23.756+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:38:23.756+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:38:23.792+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:38:23.804+0000] {standard_task_runner.py:57} INFO - Started process 713 to run task
[2023-11-25T21:38:23.810+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '396', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmp4gqygh7p']
[2023-11-25T21:38:23.817+0000] {standard_task_runner.py:85} INFO - Job 396: Subtask load_to_gcp
[2023-11-25T21:38:23.883+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:38:24.000+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:38:24.020+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 83, in load_to_gcp
    hook = GoogleCloudStorageHook(google_cloud_storage_conn_id='google_cloud_default')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 164, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 242, in __init__
    self.extras: dict = self.get_connection(self.gcp_conn_id).extra_dejson
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 477, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `google_cloud_default` isn't defined
[2023-11-25T21:38:24.026+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T213823, end_date=20231125T213824
[2023-11-25T21:38:24.040+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 396 for task load_to_gcp (The conn_id `google_cloud_default` isn't defined; 713)
[2023-11-25T21:38:24.061+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:38:24.073+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:41:28.566+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:41:28.585+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:41:28.585+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:41:28.620+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:41:28.636+0000] {standard_task_runner.py:57} INFO - Started process 776 to run task
[2023-11-25T21:41:28.648+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '398', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpc9obd4cf']
[2023-11-25T21:41:28.664+0000] {standard_task_runner.py:85} INFO - Job 398: Subtask load_to_gcp
[2023-11-25T21:41:28.771+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:41:28.904+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:41:28.930+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 83, in load_to_gcp
    hook = GoogleCloudStorageHook(google_cloud_storage_conn_id='google_cloud_storage_default')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 164, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 242, in __init__
    self.extras: dict = self.get_connection(self.gcp_conn_id).extra_dejson
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 477, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `google_cloud_default` isn't defined
[2023-11-25T21:41:28.937+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T214128, end_date=20231125T214128
[2023-11-25T21:41:28.955+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 398 for task load_to_gcp (The conn_id `google_cloud_default` isn't defined; 776)
[2023-11-25T21:41:28.984+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:41:29.006+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:43:31.670+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:43:31.680+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:43:31.680+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:43:31.697+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:43:31.705+0000] {standard_task_runner.py:57} INFO - Started process 816 to run task
[2023-11-25T21:43:31.709+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '403', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpmldcyg_z']
[2023-11-25T21:43:31.712+0000] {standard_task_runner.py:85} INFO - Job 403: Subtask load_to_gcp
[2023-11-25T21:43:31.786+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:43:31.893+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:43:31.913+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 83, in load_to_gcp
    hook = GoogleCloudStorageHook(google_cloud_storage_conn_id='google_cloud_storage_default')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 164, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 242, in __init__
    self.extras: dict = self.get_connection(self.gcp_conn_id).extra_dejson
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 477, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `google_cloud_default` isn't defined
[2023-11-25T21:43:31.918+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T214331, end_date=20231125T214331
[2023-11-25T21:43:31.944+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 403 for task load_to_gcp (The conn_id `google_cloud_default` isn't defined; 816)
[2023-11-25T21:43:31.967+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:43:31.990+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:45:02.844+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:45:02.856+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:45:02.857+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:45:02.876+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:45:02.883+0000] {standard_task_runner.py:57} INFO - Started process 844 to run task
[2023-11-25T21:45:02.889+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '404', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpk2_kxrqj']
[2023-11-25T21:45:02.895+0000] {standard_task_runner.py:85} INFO - Job 404: Subtask load_to_gcp
[2023-11-25T21:45:02.970+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:45:03.094+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:45:03.106+0000] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2023-11-25T21:45:03.107+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-11-25T21:45:03.108+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 86, in load_to_gcp
    hook.upload(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 530, in upload
    client = self.get_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 173, in get_conn
    credentials=self.get_credentials(), client_info=CLIENT_INFO, project=self.project_id
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 296, in get_credentials
    credentials, _ = self.get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 273, in get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 362, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 235, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 260, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'
[2023-11-25T21:45:03.124+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T214502, end_date=20231125T214503
[2023-11-25T21:45:03.143+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 404 for task load_to_gcp ([Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'; 844)
[2023-11-25T21:45:03.189+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:45:03.207+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:48:36.930+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:48:36.944+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:48:36.944+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:48:36.959+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:48:36.966+0000] {standard_task_runner.py:57} INFO - Started process 895 to run task
[2023-11-25T21:48:36.969+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '405', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmproom__tj']
[2023-11-25T21:48:36.971+0000] {standard_task_runner.py:85} INFO - Job 405: Subtask load_to_gcp
[2023-11-25T21:48:37.011+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:48:37.069+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:48:37.077+0000] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2023-11-25T21:48:37.078+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-11-25T21:48:37.078+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 84, in load_to_gcp
    hook.upload(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 530, in upload
    client = self.get_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 173, in get_conn
    credentials=self.get_credentials(), client_info=CLIENT_INFO, project=self.project_id
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 296, in get_credentials
    credentials, _ = self.get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 273, in get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 362, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 235, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 260, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'
[2023-11-25T21:48:37.086+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T214836, end_date=20231125T214837
[2023-11-25T21:48:37.097+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 405 for task load_to_gcp ([Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'; 895)
[2023-11-25T21:48:37.140+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:48:37.167+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:51:36.640+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:51:36.655+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:51:36.655+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:51:36.690+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:51:36.697+0000] {standard_task_runner.py:57} INFO - Started process 948 to run task
[2023-11-25T21:51:36.701+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '406', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpb3s7yo5i']
[2023-11-25T21:51:36.706+0000] {standard_task_runner.py:85} INFO - Job 406: Subtask load_to_gcp
[2023-11-25T21:51:36.790+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:51:36.879+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:51:36.889+0000] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2023-11-25T21:51:36.890+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-11-25T21:51:36.892+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 84, in load_to_gcp
    hook.upload(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 530, in upload
    client = self.get_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 173, in get_conn
    credentials=self.get_credentials(), client_info=CLIENT_INFO, project=self.project_id
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 296, in get_credentials
    credentials, _ = self.get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 273, in get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 362, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 235, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 260, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'
[2023-11-25T21:51:36.902+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T215136, end_date=20231125T215136
[2023-11-25T21:51:36.922+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 406 for task load_to_gcp ([Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'; 948)
[2023-11-25T21:51:36.975+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:51:36.998+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:54:44.343+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:54:44.360+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:54:44.360+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:54:44.388+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:54:44.403+0000] {standard_task_runner.py:57} INFO - Started process 998 to run task
[2023-11-25T21:54:44.414+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '407', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpvyc3964e']
[2023-11-25T21:54:44.419+0000] {standard_task_runner.py:85} INFO - Job 407: Subtask load_to_gcp
[2023-11-25T21:54:44.498+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:54:44.677+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:54:44.696+0000] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2023-11-25T21:54:44.697+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-11-25T21:54:44.698+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 89, in load_to_gcp
    hook.upload(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 530, in upload
    client = self.get_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 173, in get_conn
    credentials=self.get_credentials(), client_info=CLIENT_INFO, project=self.project_id
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 296, in get_credentials
    credentials, _ = self.get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 273, in get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 362, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 235, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 260, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'
[2023-11-25T21:54:44.713+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T215444, end_date=20231125T215444
[2023-11-25T21:54:44.727+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 407 for task load_to_gcp ([Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'; 998)
[2023-11-25T21:54:44.744+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:54:44.766+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-25T21:57:47.633+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:57:47.639+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [queued]>
[2023-11-25T21:57:47.639+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-11-25T21:57:47.649+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): load_to_gcp> on 2023-11-25 20:00:00+00:00
[2023-11-25T21:57:47.654+0000] {standard_task_runner.py:57} INFO - Started process 1049 to run task
[2023-11-25T21:57:47.657+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'ETL_Pipeline', 'load_to_gcp', 'scheduled__2023-11-25T20:00:00+00:00', '--job-id', '408', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Pipeline.py', '--cfg-path', '/tmp/tmpncw_1y3a']
[2023-11-25T21:57:47.658+0000] {standard_task_runner.py:85} INFO - Job 408: Subtask load_to_gcp
[2023-11-25T21:57:47.688+0000] {task_command.py:416} INFO - Running <TaskInstance: ETL_Pipeline.load_to_gcp scheduled__2023-11-25T20:00:00+00:00 [running]> on host 4012deabc79b
[2023-11-25T21:57:47.737+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_Pipeline' AIRFLOW_CTX_TASK_ID='load_to_gcp' AIRFLOW_CTX_EXECUTION_DATE='2023-11-25T20:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-11-25T20:00:00+00:00'
[2023-11-25T21:57:47.738+0000] {logging_mixin.py:154} INFO - Current Working Directory: /opt/***
[2023-11-25T21:57:47.738+0000] {logging_mixin.py:154} INFO - List of Files: ['dags', 'logs', '***-worker.pid', 'plugins', '***.cfg', 'webserver_config.py', 'config', 'requirements.txt']
[2023-11-25T21:57:47.743+0000] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2023-11-25T21:57:47.744+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-11-25T21:57:47.744+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/utils/helper.py", line 90, in load_to_gcp
    hook.upload(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 530, in upload
    client = self.get_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 173, in get_conn
    credentials=self.get_credentials(), client_info=CLIENT_INFO, project=self.project_id
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 296, in get_credentials
    credentials, _ = self.get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 273, in get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 362, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 235, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 260, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'
[2023-11-25T21:57:47.750+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Pipeline, task_id=load_to_gcp, execution_date=20231125T200000, start_date=20231125T215747, end_date=20231125T215747
[2023-11-25T21:57:47.759+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 408 for task load_to_gcp ([Errno 2] No such file or directory: '/home/siyam/Documents/protean-music-401611-34ddba62fc5c.json'; 1049)
[2023-11-25T21:57:47.788+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-11-25T21:57:47.813+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
